{
    "full_executed": false,
    "use_mpi_backend_for_parallelization_notebook.ipynb": {
        "9.7: Use MPI backend for Parallelization": {
            "level": 1,
            "html": "<div class='markdown-cell'>\n    <h1>9.7: Use MPI backend for Parallelization</h1>\n<p>This example demonstrates how to use the MPI backend for simulating\ndipoles using HNN-core.</p>\n<p>The MPI backend allows running the simulation in parallel across\nneurons in the network even with a single trial. For this, you will need\nthe <code>MPI related software &amp;lt;parallel&amp;gt;</code>\ninstalled. Note that if you want to simulate in parallel across trials,\nthe Joblib backend allows this without the need to install and configure\nMPI.</p>\n\n</div>\n<div class='code-cell'>\n    <code class='language-python'>\n        # Authors: Mainak Jas <mjas@mgh.harvard.edu>\n#          Blake Caldwell <blake_caldwell@brown.edu>\n    </code>\n</div>\n<div class='markdown-cell'>\n    <p>Let us import hnn_core</p>\n\n</div>\n<div class='code-cell'>\n    <code class='language-python'>\n        import os.path as op\n\nimport hnn_core\nfrom hnn_core import simulate_dipole, jones_2009_model\n    </code>\n</div>\n<div class='markdown-cell'>\n    <p>Following\n<code>the alpha example &amp;lt;sphx_glr_auto_examples_plot_simulate_alpha.py&amp;gt;</code>,\nwe add a ~10 Hz \"bursty\" drive starting at 50 ms and continuing to the\nend of the simulation. Each burst consists of a pair (2) of spikes,\nspaced 10 ms apart. The occurrence of each burst is jittered by a\nrandom, normally distributed amount (20 ms standard deviation). We\nrepeat the burst train 10 times, each time with unique\nrandomization.</p>\n\n</div>\n<div class='code-cell'>\n    <code class='language-python'>\n        net = jones_2009_model()\n\nweights_ampa = {'L2_pyramidal': 5.4e-5, 'L5_pyramidal': 5.4e-5}\nnet.add_bursty_drive(\n    'bursty', tstart=50., burst_rate=10, burst_std=20., numspikes=2,\n    spike_isi=10, n_drive_cells=10, location='distal',\n    weights_ampa=weights_ampa, event_seed=278)\n    </code>\n</div>\n<div class='markdown-cell'>\n    <p>Finally, to simulate we use the\n:class:<code>~hnn_core.parallel_backends.MPIBackend</code> class. This\nwill start the simulation across the number of processors (cores)\nspecified by <code>n_procs</code> using MPI. The\n<code>&amp;#x27;mpiexec&amp;#x27;</code> launcher is used from\n<code>openmpi</code>, which must be installed on the system</p>\n\n</div>\n<div class='code-cell'>\n    <code class='language-python'>\n        from hnn_core import MPIBackend\n\nwith MPIBackend(n_procs=2, mpi_cmd='mpiexec'):\n    dpls = simulate_dipole(net, tstop=310., n_trials=1)\n\ntrial_idx = 0\ndpls[trial_idx].plot()\n    </code>\n</div>"
        }
    }
}